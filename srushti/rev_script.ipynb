{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "400aadd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install evaluate\n",
    "!pip install tqdm \n",
    "!pip install gpustat\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41971c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # the device to load the model onto\n",
    "\n",
    "# specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", quantization_config=quantization_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_data = []\n",
    "dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n",
    "\n",
    "# Access the 'train' split\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Extract prompts from the 'train' split\n",
    "dataset_prompt = train_dataset['prompt']\n",
    "\n",
    "dataset_prompt = dataset_prompt[:100]\n",
    "\n",
    "def revision(principles_list):\n",
    "    for prompt in tqdm(dataset_prompt):\n",
    "        input_id = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(input_id, max_length=512, num_return_sequences=1, no_repeat_ngram_size=2,\n",
    "                                    pad_token_id=tokenizer.eos_token_id)\n",
    "        #base_answer = output[0][len(input_id[0]):].tolist()\n",
    "        base_ans = output[0].tolist()\n",
    "        base_answer = tokenizer.decode(base_ans, skip_special_tokens=True)\n",
    "        original_ans = base_answer\n",
    "        \n",
    "        #print(\"prompt: \", prompt, \"\\n\")\n",
    "        #print(\"ans:\\n\", base_answer, \"\\n\\n\")\n",
    "        #batch inference to be implemented\n",
    "   \n",
    "        for principle in principles_list:\n",
    "            critique = f\"Revise the following response with respect to {principle}: '{base_answer}'. Please be concise in your answer and try to answer in 500 tokens.\"\n",
    "        # new_prompt = f\"{critique}\"\n",
    "        # new_prompt = f\"{prompt}\\n{critique}\"\n",
    "            new_inputs = tokenizer.encode(critique, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                new_out = model.generate(new_inputs, max_length=1000, num_return_sequences=1, no_repeat_ngram_size=2,\n",
    "                                         pad_token_id=tokenizer.eos_token_id)\n",
    "            #crit_ans = new_out[0][len(new_inputs[0]):].tolist()\n",
    "            criticized_answer = tokenizer.decode(new_out[0], skip_special_tokens=True)\n",
    "            #print(\"Critique:\\n \", criticized_answer, \"\\n\")\n",
    "            #rev_prompt = f\"Revise the following response:\\n '{base_answer}'\\n and remove all the critiques listed in the following criticised answer:\\n '{criticized_answer}'\" \n",
    "            #rev_prompt1 = tokenizer.encode(rev_prompt, return_tensors='pt').to('cuda')\n",
    "            #with torch.no_grad():\n",
    "                #new_ans = model.generate(rev_prompt1, max_length=512,num_return_sequences=1, no_repeat_ngram_size=2,\n",
    "                                    # pad_token_id=tokenizer.eos_token_id)\n",
    "            #rev_a = new_ans[0][len(rev_prompt1[0]):].tolist()\n",
    "            #revised_answer = tokenizer.decode(new_ans[0], skip_special_tokens=True)\n",
    "            #print(\"rev_ans:\\n \", revised_answer, \"\\n\")\n",
    "            base_answer = criticized_answer\n",
    "        \n",
    "        revised_data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"revised_answer\": base_answer,\n",
    "        })\n",
    "    \n",
    "    with open(\"revised_datafile.json\", \"w\") as f:\n",
    "        json.dump(revised_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6686661",
   "metadata": {},
   "outputs": [],
   "source": [
    "principles_list = [\"honesty\", \"prudence\"]\n",
    "revision(principles_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
