{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "400aadd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41971c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ee06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompt = \"C:\\\\Users\\\\Tanu.DESKTOP-D6I4PBO\\\\Documents\\\\epfl\\\\bachelor project\\\\dataset\\\\10k_prompts.jsonl\"\n",
    "revised_data = []\n",
    "\n",
    "\n",
    "def revision(principles_list):\n",
    "    for prompt in dataset_prompt:\n",
    "        input_id = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(input_id, max_length=512, num_return_sequences=1, no_repeat_ngram_size=2,\n",
    "                                    pad_token_id=tokenizer.eos_token_id)\n",
    "        answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # print(\"ans:\", answer, \"\\n\\n\")\n",
    "\n",
    "    base_answer = answer\n",
    "    for principle in principles_list:\n",
    "        critique = f\"Criticize the following response with respect to {principle}: '{base_answer}'\"\n",
    "        # new_prompt = f\"{critique}\"\n",
    "        # new_prompt = f\"{prompt}\\n{critique}\"\n",
    "        new_inputs = tokenizer.encode(critique, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            new_out = model.generate(new_inputs, max_length=500, num_return_sequences=1, no_repeat_ngram_size=2,\n",
    "                                     pad_token_id=tokenizer.eos_token_id)\n",
    "        criticized_answer = tokenizer.decode(new_out[0], skip_special_tokens=True)\n",
    "        rev_prompt = f\"{base_answer}\\n{criticized_answer}\\nRemove the listed critiques and revise the following response with respect to {principle}.\"\n",
    "        rev_prompt1 = tokenizer.encode(rev_prompt, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            new_ans = model.generate(rev_prompt1, max_length=500, num_return_sequences=1, no_repeat_ngram_size=2,\n",
    "                                     pad_token_id=tokenizer.eos_token_id)\n",
    "        revised_answer = tokenizer.decode(new_ans[0], skip_special_tokens=True)\n",
    "        # print(revised_answer, \"(end)\\n\")\n",
    "\n",
    "        revised_data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"revised_answer\": revised_answer,\n",
    "            # Add any other relevant information here\n",
    "        })\n",
    "\n",
    "    with open(\"revised_datafile.json\", \"w\") as f:\n",
    "        json.dump(revised_data, f, indent=4)\n",
    "\n",
    "    return revised_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6686661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
