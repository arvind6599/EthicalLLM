{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/data/amenon-data/EthicalLLM/ethics/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import trl \n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "import os\n",
    "\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, HfArgumentParser, AutoConfig, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from trl import ModelConfig, RewardConfig, RewardTrainer, get_kbit_device_map\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the  models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/data/amenon-data/EthicalLLM/ethics/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  2.39s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "\n",
    "# Use 4 bit quantization for the main model \n",
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# lora config\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 24\n",
    "LORA_DROPOUT = 0.1\n",
    "MAX_LENGTH = 300\n",
    "batch_size = 16\n",
    "DATA_SIZE = 10000\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    r=LORA_R, \n",
    "    lora_alpha=LORA_ALPHA, \n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias='none'\n",
    ")\n",
    "\n",
    "## -------------------- LOADING THE MISTRAL MODEL AND TOKENIZER -----------\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    model_max_length=MAX_LENGTH,\n",
    "    padding=\"max_length\", \n",
    "    truncation=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "final_model_name = \"amenon/EthcalLLM-PPO-final\"\n",
    "final_model = AutoModelForCausalLM.from_pretrained(\n",
    "    final_model_name,\n",
    "    config=config,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map)\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the reward models for the scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.82s/it]\n",
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.70s/it]\n",
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.78s/it]\n",
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward models loaded\n"
     ]
    }
   ],
   "source": [
    "## -------------------- LOADING THE REWARD MODELS -------------------------------\n",
    "\n",
    "reward_models = []\n",
    "aspects = ['action', 'consequences', 'motivation']\n",
    "reward_config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", num_labels=2)\n",
    "\n",
    "for aspect in aspects:\n",
    "    reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        f\"Tachi67/EthcalLLM-RM-{aspect}\", \n",
    "        config=reward_config,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=device_map\n",
    "    )\n",
    "    reward_model.resize_token_embeddings(len(tokenizer))\n",
    "    reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
    "    reward_models.append(reward_model)\n",
    "print('Reward models loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_prompt(text):\n",
    "    # Find the index of \"\\n\\nAssistant\"\n",
    "    index = text.find(\"\\n\\nAssistant\")\n",
    "    # Extract the prompt\n",
    "    if index != -1:\n",
    "        prompt = text[:index+len(\"\\n\\nAssistant:\")]\n",
    "    else:\n",
    "        prompt = text  # If \"\\n\\nAssistant\" is not found, use the whole text as prompt\n",
    "    return prompt\n",
    "\n",
    "def preprocess_function(examples, tokenizer=tokenizer):\n",
    "        new_examples = {\n",
    "            \"query\":[],\n",
    "            \"input_ids\": [],\n",
    "            \"attention_mask\": [],\n",
    "        }\n",
    "\n",
    "        for chosen in examples[\"chosen\"]:\n",
    "            prompt = tokenize_prompt(chosen)\n",
    "            new_examples[\"query\"].append(prompt)\n",
    "            tokenized_prompt =  tokenizer(prompt, padding=True, truncation=True, return_tensors='pt')\n",
    "            new_examples[\"input_ids\"].append(tokenized_prompt[\"input_ids\"])\n",
    "            new_examples[\"attention_mask\"].append(tokenized_prompt[\"attention_mask\"])\n",
    "                \n",
    "        return new_examples\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"Tachi67/rm_data_action\")\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "prompt_test = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    )\n",
    "\n",
    "\n",
    "prompt_test = prompt_test.remove_columns([\"chosen\", \"rejected\"])\n",
    "prompt_test.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2243\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "test_dataloader = DataLoader(prompt_test, batch_size=8, shuffle=True, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 50\n",
    "def generate_additional_texts_batch(model, tokenizer, texts, prompt, max_new_tokens=MAX_NEW_TOKENS):\n",
    "    batched_inputs = [text + \" \" + prompt for text in texts]\n",
    "    input_ids = tokenizer(batched_inputs, padding=True, truncation=True, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "    # Generate response\n",
    "    outputs = model.generate(input_ids, max_new_tokens=max_new_tokens, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, tokenizer):\n",
    "    # Tokenize the batch of texts\n",
    "        tokens = tokenizer(texts, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "        return tokens\n",
    "    \n",
    "def get_reward_scores(texts, tokenizer, reward_model):\n",
    "    # Tokenize the texts\n",
    "    tokens = tokenize_texts(texts, tokenizer)\n",
    "\n",
    "    # Get the input IDs and attention mask\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens['attention_mask']\n",
    "\n",
    "    reward_scores = []\n",
    "\n",
    "    # Get model predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = reward_model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Append scores for the batch (assuming binary classification and we are interested in the probability of class 1)\n",
    "    batch_scores = probs[:, 1]\n",
    "    reward_scores.append(batch_scores)\n",
    "    \n",
    "    # # Transpose the list of scores so that each inner list corresponds to the scores of one text\n",
    "    # reward_scores_per_text = list(map(list, zip(*reward_scores)))\n",
    "    \n",
    "    return reward_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_scores(model, tokenizer, reward_models, test_dataloader):\n",
    "    scale = 0.5\n",
    "    mean_scores = []\n",
    "    for batch_idx, batch in enumerate(tqdm(test_dataloader, desc=\"Batch\")):\n",
    "    \n",
    "        scores = []\n",
    "    \n",
    "        action_prompt = \"\"\n",
    "        action = generate_additional_texts_batch(model, tokenizer, batch[\"query\"], action_prompt)\n",
    "    \n",
    "        scores.append(get_reward_scores(action, tokenizer, reward_models[0]))\n",
    "    \n",
    "        motivation_prompt = \"What is the motivation behind the Assistant's answer?\"\n",
    "        motivations = generate_additional_texts_batch(model, tokenizer, action, motivation_prompt)\n",
    "    \n",
    "        scores.append(get_reward_scores(motivations, tokenizer, reward_models[1]))\n",
    "        \n",
    "        consequence_prompt = \"What is the utility of the action towards promoting your virtuous character?\"\n",
    "        consequences = generate_additional_texts_batch(model, tokenizer, action, consequence_prompt)\n",
    "    \n",
    "        scores.append(get_reward_scores(consequences, tokenizer, reward_models[2]))\n",
    "    \n",
    "        final_scores = torch.stack((scores[0][0], scores[1][0], scores[2][0])).T\n",
    "        rewards = [s.mean() for s in final_scores]\n",
    "    \n",
    "        mean_scores += rewards\n",
    "\n",
    "    return mean_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:   4%|██▌                                                                      | 10/281 [02:17<1:02:00, 13.73s/it]"
     ]
    }
   ],
   "source": [
    "prev_scores = compute_scores(model, tokenizer, reward_models, test_dataloader)\n",
    "print(\"Previous mean = \", torch.tensor(prev_scores).mean().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "after_scores = compute_scores(final_model, tokenizer, reward_models, test_dataloader)\n",
    "print(\"Previous mean = \", torch.tensor(after_scores).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethics",
   "language": "python",
   "name": "ethics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
