{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec25ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import trl \n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "import os\n",
    "\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, HfArgumentParser, AutoConfig, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from trl import ModelConfig, RewardConfig, RewardTrainer, get_kbit_device_map\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------- Initial SETUP -------------------------------------\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "login(token=HUGGINGFACE_TOKEN)\n",
    "wandb.init()\n",
    "\n",
    "# -------------- CONFIGURATION ------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "\n",
    "# Use 4 bit quantization for the main model \n",
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# lora config\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 24\n",
    "LORA_DROPOUT = 0.1\n",
    "MAX_LENGTH = 300\n",
    "batch_size = 16\n",
    "DATA_SIZE = 10000\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    r=LORA_R, \n",
    "    lora_alpha=LORA_ALPHA, \n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias='none'\n",
    ")\n",
    "\n",
    "## -------------------- LOADING THE MISTRAL MODEL AND TOKENIZER -----------\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    model_max_length=MAX_LENGTH,\n",
    "    padding=\"max_length\", \n",
    "    truncation=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map,\n",
    "    peft_config = lora_config\n",
    ")\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "\n",
    "## -------------------- LOADING THE REWARD MODELS -------------------------------\n",
    "\n",
    "reward_models = []\n",
    "aspects = ['action', 'consequences', 'motivation']\n",
    "reward_config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", num_labels=2)\n",
    "\n",
    "for aspect in aspects:\n",
    "    reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        f\"Tachi67/EthcalLLM-RM-{aspect}\", \n",
    "        config=reward_config,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=device_map\n",
    "    )\n",
    "    reward_model.resize_token_embeddings(len(tokenizer))\n",
    "    reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
    "    reward_models.append(reward_model)\n",
    "print('Reward models loaded')\n",
    "\n",
    "\n",
    "# ----------- LOADING THE PROMPTS FROM THE REWARD DATASETS --------------------- \n",
    "\n",
    "\n",
    "def tokenize_prompt(text):\n",
    "    # Find the index of \"\\n\\nAssistant\"\n",
    "    index = text.find(\"\\n\\nAssistant\")\n",
    "    # Extract the prompt\n",
    "    if index != -1:\n",
    "        prompt = text[:index+len(\"\\n\\nAssistant:\")]\n",
    "    else:\n",
    "        prompt = text  # If \"\\n\\nAssistant\" is not found, use the whole text as prompt\n",
    "    return prompt\n",
    "\n",
    "def preprocess_function(examples, tokenizer=tokenizer):\n",
    "        new_examples = {\n",
    "            \"query\":[],\n",
    "            \"input_ids\": [],\n",
    "            \"attention_mask\": [],\n",
    "        }\n",
    "\n",
    "        for chosen in examples[\"chosen\"]:\n",
    "            prompt = tokenize_prompt(chosen)\n",
    "            new_examples[\"query\"].append(prompt)\n",
    "            tokenized_prompt =  tokenizer(prompt, padding=True, truncation=True, return_tensors='pt')\n",
    "            new_examples[\"input_ids\"].append(tokenized_prompt[\"input_ids\"])\n",
    "            new_examples[\"attention_mask\"].append(tokenized_prompt[\"attention_mask\"])\n",
    "                \n",
    "        return new_examples\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"Tachi67/rm_data_action\")\n",
    "train_dataset = dataset['train'].select(range(DATA_SIZE))\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "prompt_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    )\n",
    "\n",
    "prompt_test = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    )\n",
    "\n",
    "prompt_train = prompt_train.remove_columns([\"chosen\", \"rejected\"])\n",
    "prompt_test = prompt_test.remove_columns([\"chosen\", \"rejected\"])\n",
    "\n",
    "prompt_train.set_format(type=\"torch\")\n",
    "prompt_test.set_format(type=\"torch\")\n",
    "\n",
    "# ----------------- PPO Trainer ---------------------------\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with=\"wandb\",\n",
    "    batch_size=batch_size,\n",
    "    mini_batch_size=1,\n",
    "    gradient_accumulation_steps=batch_size,\n",
    "    optimize_cuda_cache=True,\n",
    "    target_kl=0.1,\n",
    "    ppo_epochs=1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "# Define the PPO taii\n",
    "ppo_trainer = PPOTrainer(\n",
    "    ppo_config,\n",
    "    model,\n",
    "    ref_model=None,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=prompt_train,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2fcbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, tokenizer):\n",
    "    # Tokenize the batch of texts\n",
    "    tokens = tokenizer(texts, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "    return tokens\n",
    "\n",
    "def get_reward_scores(texts, tokenizer, reward_model):\n",
    "    # Tokenize the texts\n",
    "    tokens = tokenize_texts(texts, tokenizer)\n",
    "\n",
    "    # Get the input IDs and attention mask\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens['attention_mask']\n",
    "\n",
    "    reward_scores = []\n",
    "\n",
    "    # Get model predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = reward_model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Append scores for the batch (assuming binary classification and we are interested in the probability of class 1)\n",
    "    batch_scores = probs[:, 1]\n",
    "    reward_scores.append(batch_scores)\n",
    "    \n",
    "    # # Transpose the list of scores so that each inner list corresponds to the scores of one text\n",
    "    # reward_scores_per_text = list(map(list, zip(*reward_scores)))\n",
    "    \n",
    "    return reward_scores\n",
    "\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1, # don't ignore the EOS token (see above)\n",
    "    \"top_k\": 0.0, # no top-k sampling\n",
    "    \"top_p\": 1.0, # no nucleus sampling\n",
    "    \"do_sample\": True, # yes, we want to sample\n",
    "    \"pad_token_id\": tokenizer.eos_token_id, # most decoder models don't have a padding token - use EOS token instead\n",
    "    \"max_new_tokens\": 32, # specify how many tokens you want to generate at most\n",
    "}\n",
    "scale = 0.5 #factor to take the std into account to mitigate overoptimization\n",
    "\n",
    "start = time.time()\n",
    "print(\"Training...\")\n",
    "\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "epochs = 1\n",
    "\n",
    "save_interval = 100  # Save checkpoint every 100 batches\n",
    "\n",
    "\n",
    "# Function to generate additional texts (motivation and consequence) in batches\n",
    "def generate_additional_texts_batch(model, tokenizer, texts, prompt, max_new_tokens):\n",
    "    batched_inputs = [text + \" \" + prompt for text in texts]\n",
    "    input_ids = tokenizer(batched_inputs, padding=True, truncation=True, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "    # Generate response\n",
    "    outputs = model.generate(input_ids, max_new_tokens=max_new_tokens, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    return generated_texts\n",
    "\n",
    "\n",
    "for batch_idx, batch in enumerate(tqdm(ppo_trainer.dataloader, desc=\"Batch\")):\n",
    "    \n",
    "\n",
    "    # Concatenate list of flattened tensors containing the input ids\n",
    "    query_tensors = [ t.flatten() for t in batch['input_ids']]\n",
    "    \n",
    "    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "\n",
    "    new_token_responses = [response[len(query_tensors[i]):] for i, response in enumerate(response_tensors)]        \n",
    "    # Decode the new tokens\n",
    "    batch[\"response\"] = [tokenizer.decode(new_tokens, skip_special_tokens=True) for new_tokens in response_tensors]\n",
    "\n",
    "    scores = get_reward_scores(batch[\"response\"], tokenizer, reward_models[0])\n",
    "\n",
    "    # Generate motivations and consequences in batch\n",
    "    motivation_prompt = \"What is the motivation behind the Assistant's answer?\"\n",
    "    motivations = generate_additional_texts_batch(ppo_trainer.model, tokenizer, batch[\"response\"], motivation_prompt, max_new_tokens=32)\n",
    "    \n",
    "    consequence_prompt = \"What is the utility of the action towards promoting your virtuous character?\"\n",
    "    consequences = generate_additional_texts_batch(ppo_trainer.model, tokenizer, batch[\"response\"], consequence_prompt, max_new_tokens=32)\n",
    "    \n",
    "    # Decode the motivations and consequences\n",
    "    batch[\"motivation\"] = motivations\n",
    "    batch[\"consequence\"] = consequences\n",
    "    \n",
    "    motivation_scores = get_reward_scores(motivations, tokenizer, reward_models[1])\n",
    "    consequence_scores = get_reward_scores(consequences, tokenizer, reward_models[2])\n",
    "    \n",
    "    \n",
    "    final_scores = torch.stack((scores[0], motivation_scores[0], consequence_scores[0])).T\n",
    "    rewards = [s.mean() - scale*(s.std()) for s in final_scores]\n",
    "\n",
    "    stats = ppo_trainer.step(query_tensors, new_token_responses, rewards)\n",
    "    # ppo_trainer.log_stats(stats, batch, rewards)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed702029",
   "metadata": {},
   "source": [
    "## Load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOConfig\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=1.41e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11323242",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a98357",
   "metadata": {},
   "source": [
    "## Load the dataset to generate query and response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb61645",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = pd.read_csv(\"./output/responses.csv\")['Response']\n",
    "response_text = pd.read_csv(\"./output/response_text.csv\")\n",
    "\n",
    "s = response_text.iloc[0]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7820758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_answer(s, print_values=False):\n",
    "    \n",
    "    find_str = '# Answer :'\n",
    "    ind = s.find(find_str)\n",
    "    l = len(find_str)\n",
    "    \n",
    "    ab = s[ind+l+1:ind+l+2]\n",
    "    opab = s[ind+l+8:ind+l+9]\n",
    "    \n",
    "    if print_values:\n",
    "        print(ab,'\\n', opab)\n",
    "    \n",
    "    if ab == 'A' or ab == 'B':\n",
    "        return s[:ind], ab, s[ind+l+2:]\n",
    "    elif opab == 'A' or opab == 'B':\n",
    "        return s[:ind], opab, s[ind+l+9:]\n",
    "    else:\n",
    "        return [s[:ind], 'NA', s[ind+l+9:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f888d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_str = '# Answer :'\n",
    "ind = s.find(find_str)\n",
    "\n",
    "\n",
    "query, answer, reason = find_answer(s)\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer, reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b90313",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = response_text['Text'].apply(find_answer).apply(pd.Series)\n",
    "values.columns = ['Query', 'Answer', 'Reason']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8464e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbcd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "values.to_csv('./output/query-response-pairs.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1193001",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = pd.read_csv('./output/query-response-pairs.csv')\n",
    "\n",
    "query_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c59569",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE TO LOAD LLAMA-3 8B model\n",
    "\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n",
    "# )\n",
    "# pipeline(\"Hey how are you doing today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190c25d",
   "metadata": {},
   "source": [
    "## PPO trainer example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57963e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/cherry_picked_prompts\", split=\"train\")\n",
    "dataset = dataset.rename_column(\"prompt\", \"query\")\n",
    "dataset = dataset.remove_columns([\"meta\", \"completion\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bdc2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cedeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOConfig\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=1.41e-5,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d68172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "reward_model = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\")\n",
    "\n",
    "\n",
    "## - ------------------- Define the model here -------\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"])\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1926c9c5",
   "metadata": {},
   "source": [
    "# Define the PPO trainer and the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f93c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddeba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa94a5",
   "metadata": {},
   "source": [
    "## Training Loop using the throuple [QUERY, RESPONSE, REWARDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f8716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "for epoch in tqdm(range(epochs), \"epoch: \"):\n",
    "    for batch in tqdm(ppo_trainer.dataloader): \n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "    \n",
    "        #### Get response from SFTModel\n",
    "        response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "        batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "    \n",
    "        #### Compute reward score\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "        pipe_outputs = reward_model(texts)\n",
    "        rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "    \n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "#### Save model\n",
    "ppo_trainer.save_pretrained(\"my_ppo_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
