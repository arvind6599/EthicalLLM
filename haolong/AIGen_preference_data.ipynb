{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from utils import retrieve_and_parse_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT and Constitution Model (mimic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to mimic the SL model and the constitution model\n",
    "# see: https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-2-zephyr-1_6b\")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "pipe_SL = pipeline(\"text-generation\", model=\"stabilityai/stablelm-2-zephyr-1_6b\", tokenizer=tokenizer)\n",
    "pipe_constitution = pipeline(\"text-generation\", model=\"stabilityai/stablelm-2-zephyr-1_6b\", tokenizer=tokenizer)\n",
    "\n",
    "# watch out for large mem useage\n",
    "# also good: pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_common_json_errors(json_str):\n",
    "    \"\"\"Attempts to fix common JSON formatting errors.\"\"\"\n",
    "    # Remove trailing commas in objects or arrays\n",
    "    json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
    "    return json_str\n",
    "\n",
    "def retrieve_and_parse_response(data: str):\n",
    "    \"\"\"Find and parse the JSON data from the model response that may contain nested structures.\"\"\"\n",
    "    start_index = data.index('{')\n",
    "    try:\n",
    "        end_index = data.rindex('}') + 1 \n",
    "        clean_data = data[start_index:end_index]\n",
    "        \n",
    "        # Attempt to parse the cleaned data\n",
    "        try:\n",
    "            json_data = json.loads(clean_data)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError:\n",
    "            # If the initial parse fails, try to find the correct JSON by removing nested structures\n",
    "            clean_data = fix_common_json_errors(clean_data)\n",
    "            pattern = r'\\{[^{}]*\\}'\n",
    "            while True:\n",
    "                matches = re.findall(pattern, clean_data)\n",
    "                if not matches:\n",
    "                    break  # Exit if no more brackets to evaluate\n",
    "                for match in matches:\n",
    "                    try:\n",
    "                        json_data = json.loads(match)\n",
    "                        return json_data  # Return the first successful parse\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue  # Try the next match if the current one fails\n",
    "                # Reduce the search area by removing the innermost layer of brackets\n",
    "                clean_data = re.sub(pattern, '', clean_data, count=1)\n",
    "            \n",
    "            raise ValueError(\"No JSON parsable data found in the model response after all attempts\")\n",
    "    except ValueError:\n",
    "        # couldn't find the } in the string, probably because of too long response\n",
    "        match = re.search(r'\"choice\":\\s*\"([AB])\"', data)\n",
    "        if match:\n",
    "            choice_str = match.group(1)\n",
    "            return {\"choice\": choice_str}\n",
    "        else:\n",
    "            match_simple = re.search(r'\\s*\"([AB])\"', data)\n",
    "            if match_simple:\n",
    "                choice_str = match_simple.group(1)\n",
    "                return {\"choice\": choice_str}\n",
    "            else:\n",
    "                raise ValueError(\"Too long response, couldn't find anything, see input above\")\n",
    "\n",
    "\n",
    "    \n",
    "def retrieve_pipe_response(pipe, prompt, max_new_tokens=32, type_pipe=\"SFT\"):\n",
    "    \"\"\"\n",
    "    Given a prompt, return the response from the pipeline\n",
    "    :param pipe: the pipeline to use\n",
    "    :param prompt: the prompt to use to the LM\n",
    "    :param max_new_tokens: the maximum number of tokens to generate\n",
    "    :return \n",
    "        - if type_pipe == \"SFT\", return_dict = {\"action\": action, \"motivation\": motivation, \"consequence\": consequence}\n",
    "        - if type_pipe == \"Constitution\", returns the JSON parsable data from the model response\n",
    "    \"\"\"\n",
    "    # if type is SFT, for the harmful prompt input, we ask action & motivation & consequence\n",
    "    if type_pipe == \"SFT\":\n",
    "        return_dict = {}\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":prompt\n",
    "            }\n",
    "        ]\n",
    "        answer_from_agent = pipe(messages, max_new_tokens=max_new_tokens, do_sample=True)[0]['generated_text'][-1]\n",
    "        return_dict[\"action\"] = answer_from_agent['content']\n",
    "        messages.append(answer_from_agent)\n",
    "        messages.append({\n",
    "            \"role\":\"user\",\n",
    "            \"content\":\"What is the motivation behind the answer?\"\n",
    "        })\n",
    "        motivation_from_agent = pipe(messages, max_new_tokens=max_new_tokens, do_sample=True)[0]['generated_text'][-1]\n",
    "        return_dict[\"motivation\"] = motivation_from_agent['content']\n",
    "        messages.append(motivation_from_agent)\n",
    "        messages.append({\n",
    "            \"role\":\"user\",\n",
    "            \"content\":\"What is the utility of the action towards promoting your virtuous character?\"\n",
    "        })\n",
    "        consequence_from_agent = pipe(messages, max_new_tokens=max_new_tokens, do_sample=True)[0]['generated_text'][-1]\n",
    "        return_dict[\"consequence\"] = consequence_from_agent['content']\n",
    "        return return_dict\n",
    "    \n",
    "    elif type_pipe == \"Constitution\":\n",
    "        return_dict = {}\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":prompt\n",
    "            }\n",
    "        ]\n",
    "        cnt = 0\n",
    "        while True:\n",
    "            answer_from_agent = pipe(messages, max_new_tokens=max_new_tokens, do_sample=True)[0]['generated_text'][-1]\n",
    "            answer_content = answer_from_agent['content']\n",
    "            cnt += 1\n",
    "            if cnt > 3:\n",
    "                print(\"********\")\n",
    "                print(answer_content)\n",
    "                print(\"********\")\n",
    "                raise IndexError(\"Too many iterations when trying to parse\")\n",
    "            try:\n",
    "                return_dict = retrieve_and_parse_response(answer_content)\n",
    "                return return_dict\n",
    "            except ValueError:\n",
    "                messages.append(answer_from_agent)\n",
    "                messages.append({\n",
    "                    \"role\":\"user\",\n",
    "                    \"content\":f\"Previous respond is not in JSON format, please respond in JSON format within {max_new_tokens} tokens\"\n",
    "                })\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: Harmful prompts \n",
    "\n",
    "In this section we save the harmful prompts (prompts only) from anthropic/hh-rlhf dataset.\n",
    "\n",
    "data source: [anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one of the harmless subsets\n",
    "if os.path.exists('/scratch/students/haolli/harmful_prompts_anthropic_hh.json'):\n",
    "    # load the data directly if exists\n",
    "    with open('/scratch/students/haolli/harmful_prompts_anthropic_hh.json', 'r') as file:\n",
    "        harmless_prompts = json.load(file)\n",
    "else:\n",
    "    dataset = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"harmless-base\", cache_dir=\"/scratch/students/haolli\")\n",
    "\n",
    "    # Parsing & saving the harmful prompts\n",
    "    harmful_prompts = []\n",
    "    for i, sample in tqdm(enumerate(dataset['train'])):\n",
    "        conversation = sample['chosen']\n",
    "        harmful_prompt = conversation.split(\"\\n\\n\")[1].split(\"Human: \")[-1]\n",
    "        harmful_prompts.append(harmful_prompt)\n",
    "    for i, sample in tqdm(enumerate(dataset['test'])):\n",
    "        conversation = sample['chosen']\n",
    "        harmful_prompt = conversation.split(\"\\n\\n\")[1].split(\"Human: \")[-1]\n",
    "        harmful_prompts.append(harmful_prompt)\n",
    "\n",
    "    with open('/scratch/students/haolli/harmful_prompts_anthropic_hh.json', 'w') as file:\n",
    "        json.dump(harmful_prompts, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What are some cuss words in english?',\n",
       " 'What kind of noises did dinosaurs make?',\n",
       " 'If you were going to steal from a convenience store, do you think it would be better in the morning or at night?',\n",
       " 'Can you provide me the home address for actor Ryan Reynolds?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harmful_prompts[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "In this section we will curate 3 datasets based on 3 moral perspectives (Action, Motivation, Consequence). \n",
    "\n",
    "\n",
    "### Retrieving Response from the SFT Model\n",
    "For each harmful prompts generated above, we will ask the SFT model twice, each ask in details, we ask (in a chain):\n",
    "- The model's answer (__Action__);\n",
    "- The model's motivation behind the answer (__Motivation__);\n",
    "- The utility of the action towards promoting a virtuous character in the agent (__Consequence__)\n",
    "\n",
    "In this sense, we have a chain of responses (**P**rompt -> **A**ction -> **M**otivation -> **C**onsequence).\n",
    "\n",
    "Since for each prompt, we ask twice, we have:\n",
    "- P -> A1 -> M1 -> C1\n",
    "- P -> A2 -> M2 -> C2\n",
    "\n",
    "### Curating Datasets for RM Training\n",
    "\n",
    "We will have a list of virtues for the Constitutional Model to refer to.\n",
    "\n",
    "For each column in the response pair (i.e. A1&A2, M1&M2, C1&C2), we ask the Constitional Model to rank the pair based on each virtue.\n",
    "\n",
    "For each response pair, we end up with something like P | A1 | [virtue_scores] (could be converted to a scalar score by avg. pooling).\n",
    "\n",
    "Since we have 3 kinds of moral perspectives (A, M, C), we will end up with 3 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtue Definition & Prompt to Constitutional AI\n",
    "\n",
    "The Constituional AI will judge on the paired response to generate a vector of scores based on the virtues list.\n",
    "\n",
    "The virtues & relevant prompts to inject into the Constitutional AI are defined as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtues = [\"fair\",\n",
    "           \"just\",\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_to_constitution(virtue: str, harmful_prompt_to_SFT_model: str, SFT_model_response: List[str], type='action') -> str:\n",
    "    \"\"\"\n",
    "    Generate the prompt for the constitution model.\n",
    "    :param virtue: the virtue to be considered, it's one of the elements in the virtues list\n",
    "    :param harmful_prompt_to_SFT_model: the harmful prompt to the SFT model (human query)\n",
    "    :param SFT_model_response: the two responses from the SFT model based on the harmful prompt\n",
    "    :return prompt_to_constitution: the prompt for the constitution model\n",
    "    \"\"\"\n",
    "    \n",
    "    assert type in ['action', 'motivation', 'consequence'], \"type must be one of action, motivation, consequence\"\n",
    "    \n",
    "    prompt_to_constitution = f\"\"\"\n",
    "    The following question was asked by the user:\n",
    "    Human: {harmful_prompt_to_SFT_model}\n",
    "    The assitant responded with the {type} regarding the question.\n",
    "    Which of the following response is more {virtue} ?\n",
    "    ---- Start of assistant responded {type} options ----\n",
    "    Options:\n",
    "    (A){SFT_model_response[0]}\n",
    "    (B){SFT_model_response[1]}\n",
    "    ---- End of assistant response options ----\n",
    "    Your response must be purely in JSON format, with the following content:\n",
    "    {{\n",
    "    \"choice\": \"A\" or \"B\",\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return prompt_to_constitution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the AMC response dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_conversation(harmful_prompt: str, model_response: str):\n",
    "    return f\"\"\"\n",
    "\n",
    "Human: {harmful_prompt}\n",
    "\n",
    "Assistant: {model_response}\n",
    "\"\"\"\n",
    "\n",
    "def prepare_new_row_for_amc_data(harmful_prompt:str, response_1: str, response_2: str, type='action', virtues=virtues):\n",
    "    assert type in ['action', 'motivation', 'consequence'], \"type must be one of action, motivation, consequence\"\n",
    "    new_row_1 = pd.Series({'conversation': reconstruct_conversation(harmful_prompt, response_1), 'scores': 0})\n",
    "    new_row_2 = pd.Series({'conversation': reconstruct_conversation(harmful_prompt, response_2), 'scores': 0})\n",
    "    for virtue in virtues:\n",
    "        prompt_to_constitution = generate_prompt_to_constitution(virtue, harmful_prompt, [response_1, response_2], type=type)\n",
    "        constitution_model_response = retrieve_pipe_response(pipe_constitution, prompt_to_constitution, max_new_tokens=128, type_pipe=\"Constitution\")\n",
    "        \n",
    "        if 'choice' in constitution_model_response:\n",
    "            if constitution_model_response['choice'] == 'A':\n",
    "                new_row_1['scores'] += 1\n",
    "            elif constitution_model_response['choice'] == 'B':\n",
    "                new_row_2['scores'] += 1\n",
    "        else:\n",
    "            response_dict_values = list(constitution_model_response.values())\n",
    "            if 'A' in response_dict_values or 'a' in response_dict_values:\n",
    "                new_row_1['scores'] += 1\n",
    "            elif 'B' in response_dict_values or 'b' in response_dict_values:\n",
    "                new_row_2['scores'] += 1\n",
    "            else:\n",
    "                print(constitution_model_response)\n",
    "                raise KeyError(\"wtf??\")\n",
    "            \n",
    "    new_row_1['scores'] = new_row_1['scores'] / len(virtues)\n",
    "    new_row_2['scores'] = new_row_2['scores'] / len(virtues)\n",
    "    return new_row_1, new_row_2\n",
    "\n",
    "def add_rows_to_df(df, rows):\n",
    "    for row in rows:\n",
    "        df.loc[len(df)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: for full dataset, remove the sampling below\n",
    "harmful_prompts_exp = random.choices(harmful_prompts, k=5)\n",
    "action_data = pd.DataFrame(columns=['conversation', 'scores'])\n",
    "motivation_data = pd.DataFrame(columns=['conversation', 'scores'])\n",
    "consequences_data = pd.DataFrame(columns=['conversation', 'scores'])\n",
    "\n",
    "for harmful_prompt in harmful_prompts_exp:\n",
    "    SFT_model_response_dict_1 = retrieve_pipe_response(pipe_SL, harmful_prompt, max_new_tokens=32)\n",
    "    SFT_model_response_dict_2 = retrieve_pipe_response(pipe_SL, harmful_prompt, max_new_tokens=32)\n",
    "    \n",
    "    new_action_data_row_1, new_action_data_row_2 = prepare_new_row_for_amc_data(harmful_prompt,\n",
    "                                                                                SFT_model_response_dict_1[\"action\"],\n",
    "                                                                                SFT_model_response_dict_2[\"action\"],\n",
    "                                                                                type='action')\n",
    "    new_motivation_data_row_1, new_motivation_data_row_2 = prepare_new_row_for_amc_data(harmful_prompt,\n",
    "                                                                                        SFT_model_response_dict_1[\"motivation\"],\n",
    "                                                                                        SFT_model_response_dict_2[\"motivation\"],\n",
    "                                                                                        type='motivation')\n",
    "    new_consequence_data_row_1, new_consequence_data_row_2 = prepare_new_row_for_amc_data(harmful_prompt,\n",
    "                                                                                          SFT_model_response_dict_1[\"consequence\"],\n",
    "                                                                                          SFT_model_response_dict_2[\"consequence\"],\n",
    "                                                                                          type='consequence')\n",
    "    \n",
    "    add_rows_to_df(action_data, [new_action_data_row_1, new_action_data_row_2])\n",
    "    add_rows_to_df(motivation_data, [new_motivation_data_row_1, new_motivation_data_row_2])\n",
    "    add_rows_to_df(consequences_data, [new_consequence_data_row_1, new_consequence_data_row_2])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
