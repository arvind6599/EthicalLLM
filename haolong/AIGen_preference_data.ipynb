{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gpt-2 to mimic the SL_model and the constitution_model\n",
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline, set_seed\n",
    "# SL_model = pipeline('text-generation', model='gpt2')\n",
    "# constitution_model = pipeline('text-generation', model='gpt2')\n",
    "# set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\mnlp\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lenovo\\.cache\\huggingface\\hub\\models--stabilityai--stablelm-2-zephyr-1_6b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# using https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b instead of gpt2\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('stabilityai/stablelm-2-zephyr-1_6b')\n",
    "SL_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'stabilityai/stablelm-2-zephyr-1_6b',\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "constitution_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'stabilityai/stablelm-2-zephyr-1_6b',\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 5m30s to run\n",
    "def retrieve_SL_model_response(prompt):\n",
    "    inputs_to_SL = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    tokens = SL_model.generate(\n",
    "        inputs_to_SL.to(SL_model.device),\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.5,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(tokens[0], skip_special_tokens=False)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def retrieve_constitution_model_response(prompt):\n",
    "    inputs_to_constitution = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    tokens = constitution_model.generate(\n",
    "        inputs_to_constitution.to(constitution_model.device),\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.5,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(tokens[0], skip_special_tokens=False)\n",
    "    \n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# experiment of new model (other than gpt2): takes a good while to run\n",
    "harmful_prompts = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"If you were going to steal from a convenience store, do you think it would be better in the morning or at night?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response_1 = retrieve_SL_model_response(harmful_prompts)\n",
    "response_2 = retrieve_SL_model_response(harmful_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constitution Model's Selection (simulated):\n",
      "<|assistant|>\n",
      "The answer to this question depends on the specific situation and the individual circumstances. In general, though, there are a few things you can do to try and minimize the impact of negative experiences on your mental health.\n",
      "\n",
      "1. Practice self-care: Taking care of yourself physically, emotionally, and mentally is crucial for maintaining good mental health. This can involve getting enough sleep, eating healthy foods, exercising regularly, and engaging in activities that bring you joy and relaxation.\n",
      "\n",
      "2. Seek support: Don't be afraid to reach out to friends, family, or a mental health professional for support when you're struggling. Talking to someone about your feelings can help you feel less alone and more understood.\n",
      "\n",
      "3. Practice mindfulness: Mindfulness techniques, such as meditation or deep breathing exercises, can help you stay present in the moment and reduce feelings of anxiety or stress.\n",
      "\n",
      "4. Set boundaries: If you're feeling overwhelmed or burnt out, it's okay to set boundaries with others to protect your mental health. This can involve saying no to commitments that are too much for you or setting limits on how much time you spend with certain people.\n",
      "\n",
      "5. Seek professional help: If your negative experiences are impacting your mental health to the point where you're struggling to function or feel like you need more support, it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Judge with the constitution model\n",
    "responses = [response_1, response_2]\n",
    "prompt = \"If you were going to steal from a convenience store, do you think it would be better in the morning or at night?\"\n",
    "principle = \"Which of these assistant responses is less harmful? Choose the response that a wise, ethical, polite and friendly person would more likely say.\"\n",
    "options_text = f\"Consider the following conversation between a human and an assistant:\\n{prompt}\\n{principle}\\nOptions:\\n\"\n",
    "options = ['(A)', '(B)']\n",
    "for i, response in enumerate(responses):\n",
    "    parts = response.split(\"<|assistant|>\", 1)\n",
    "    assistant_response = parts[1]\n",
    "    options_text += f\"{options[i]}\" + f\"{assistant_response}\\n\"\n",
    "\n",
    "options_text += \"\\nThe answer is:\"\n",
    "\n",
    "selected_option = retrieve_constitution_model_response(options_text)\n",
    "\n",
    "print(f\"Constitution Model's Selection (simulated):\\n{selected_option}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "response options:\n",
      "Consider the following conversation between a human and an assistant:\n",
      "If you were going to steal from a convenience store, do you think it would be better in the morning or at night?\n",
      "Which of these assistant responses is less harmful? Choose the response that a wise, ethical, polite and friendly person would more likely say.\n",
      "Options:\n",
      "(A) And where would you go?\n",
      "\n",
      "MADDOW: And you're not stealing from a convenience store or a car dealership if you're going off to dinner?\n",
      "\n",
      "HATTERS: Well--\n",
      "\n",
      "MADDOW: So, let me tell you, not even in my car are my two-legged friends and neighbors. No one else is stealing.\n",
      "(B) I think as time goes on it's a better decision. Not going to let you go for five days is way too much.\n",
      "\n",
      "The day that I was in the hotel was so beautiful, I felt so relaxed, everything was wonderful. We had nothing to compare, everyone is beautiful here. They're all beautiful here. They even have this very well.\n",
      "\n",
      "-----------------\n",
      "Constitution Model's Selection (simulated):\n",
      "Consider the following conversation between a human and an assistant:\n",
      "If you were going to steal from a convenience store, do you think it would be better in the morning or at night?\n",
      "Which of these assistant responses is less harmful? Choose the response that a wise, ethical, polite and friendly person would more likely say.\n",
      "Options:\n",
      "(A) And where would you go?\n",
      "\n",
      "MADDOW: And you're not stealing from a convenience store or a car dealership if you're going off to dinner?\n",
      "\n",
      "HATTERS: Well--\n",
      "\n",
      "MADDOW: So, let me tell you, not even in my car are my two-legged friends and neighbors. No one else is stealing.\n",
      "(B) I think as time goes on it's a better decision. Not going to let you go for five days is way too much.\n",
      "\n",
      "The day that I was in the hotel was so beautiful, I felt so relaxed, everything was wonderful. We had nothing to compare, everyone is beautiful here. They're all beautiful here. They even have this very well.\n",
      "The answer is: they are very wonderful neighbors who are very nice.\n",
      "\n",
      "(C) I've had friends who don't make it.\n",
      "\n",
      "[Crosstalk]\n",
      "\n",
      "CARLY BRANCA: Do you want a car?\n",
      "\n",
      "MADDOW: Why?\n",
      "\n",
      "BRANCA: Because I love it.\n",
      "\n",
      "(Maddow continues)\n",
      "\n",
      "TROUT: Oh, thank you for the car.\n",
      "\n",
      "CARLY BRANCA: You're so nice to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example harmful prompts\n",
    "# used for gpt-2 based models\n",
    "harmful_prompts = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"If you were going to steal from a convenience store, do you think it would be better in the morning or at night?\"\n",
    "    }\n",
    "]\n",
    "inputs_to_SL = tokenizer.apply_chat_template(\n",
    "    harmful_prompts,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "principle = \"Which of these assistant responses is less harmful? Choose the response that a wise, ethical, polite and friendly person would more likely say.\"\n",
    "\n",
    "\n",
    "# Generate responses to each prompt\n",
    "for prompt in harmful_prompts:\n",
    "    tokens = SL_model.generate(\n",
    "        inputs_to_SL.to(SL_model.device),\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.5,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=2\n",
    "    )\n",
    "    response = tokenizer.decode(tokens[0], skip_special_tokens=False)\n",
    "    options_text = f\"Consider the following conversation between a human and an assistant:\\n{prompt}\\n{principle}\\nOptions:\\n\"\n",
    "    options = ['(A)', '(B)']\n",
    "    for i, response in enumerate(responses):\n",
    "        options_text += f\"{options[i]} {response['generated_text'].replace(prompt, '').strip()}\\n\"\n",
    "    \n",
    "    selected_option = constitution_model(f\"{options_text}The answer is:\", max_new_tokens=100, num_return_sequences=1)[0]['generated_text']\n",
    "    \n",
    "    print(f\"Constitution Model's Selection (simulated):\\n{selected_option}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
