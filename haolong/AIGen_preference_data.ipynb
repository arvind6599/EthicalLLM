{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import random\n",
    "from utils import retrieve_and_parse_response, load_harmful_data, reconstruct_conversation, add_rows_to_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT and Constitution Model (mimic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to mimic the SL model and the constitution model\n",
    "# see: https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-2-zephyr-1_6b\")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "pipe_SL = pipeline(\"text-generation\", model=\"stabilityai/stablelm-2-zephyr-1_6b\", tokenizer=tokenizer)\n",
    "pipe_constitution = pipeline(\"text-generation\", model=\"stabilityai/stablelm-2-zephyr-1_6b\", tokenizer=tokenizer)\n",
    "\n",
    "# watch out for large mem useage\n",
    "# also good: pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pipe_response(pipe, prompt, max_new_tokens=32, type_pipe=\"SFT\"):\n",
    "    \"\"\"\n",
    "    Given a prompt, return the response from the pipeline\n",
    "    :param pipe: the pipeline to use\n",
    "    :param prompt: the prompt to use to the LM\n",
    "    :param max_new_tokens: the maximum number of tokens to generate\n",
    "    :return \n",
    "        - if type_pipe == \"SFT\", return_dict = {\"action\": action, \"motivation\": motivation, \"consequence\": consequence}\n",
    "        - if type_pipe == \"Constitution\", returns the JSON parsable data from the model response\n",
    "    \"\"\"\n",
    "    # If type is SFT, for the harmful prompt input, we ask action & motivation & consequence\n",
    "    # We ask the 3 questions sequentially by appending the chat history & user query\n",
    "    if type_pipe == \"SFT\":\n",
    "        return_dict = {}\n",
    "        # retriving action\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":prompt\n",
    "            }\n",
    "        ]\n",
    "        answer_from_agent = pipe(messages, max_new_tokens=max_new_tokens, do_sample=True)[0]['generated_text'][-1]\n",
    "        return_dict[\"action\"] = answer_from_agent['content']\n",
    "        \n",
    "        # retriving motivation\n",
    "        messages.append(answer_from_agent)\n",
    "        messages.append({\n",
    "            \"role\":\"user\",\n",
    "            \"content\":\"What is the motivation behind the answer?\"\n",
    "        })\n",
    "        motivation_from_agent = pipe(messages, max_new_tokens=max_new_tokens, do_sample=True)[0]['generated_text'][-1]\n",
    "        return_dict[\"motivation\"] = motivation_from_agent['content']\n",
    "        \n",
    "        # retriving concequence\n",
    "        messages.append(motivation_from_agent)\n",
    "        messages.append({\n",
    "            \"role\":\"user\",\n",
    "            \"content\":\"What is the utility of the action towards promoting your virtuous character?\"\n",
    "        })\n",
    "        consequence_from_agent = pipe(messages, max_new_tokens=max_new_tokens, do_sample=True)[0]['generated_text'][-1]\n",
    "        return_dict[\"consequence\"] = consequence_from_agent['content']\n",
    "        return return_dict\n",
    "    \n",
    "    # If type is constitution, we ask the constitution model of the prompts generated from generate_prompt_to_constitution (see below)\n",
    "    # Most of the work here relates to paring the output from the model (see below, also see utils.py/retrieve_and_parse_response)\n",
    "    elif type_pipe == \"Constitution\":\n",
    "        return_dict = {}\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":prompt\n",
    "            }\n",
    "        ]\n",
    "        # If output is cannot be handled by the parser, we ask the LM again for a valid response\n",
    "        # We don't keep asking in case the model simply fails to do so and stop.\n",
    "        # TODO: for verbosity, maybe consider skipping the current instance if things dont work out now.\n",
    "        cnt = 0\n",
    "        while True:\n",
    "            answer_from_agent = pipe(messages, max_new_tokens=max_new_tokens, do_sample=True)[0]['generated_text'][-1]\n",
    "            answer_content = answer_from_agent['content']\n",
    "            cnt += 1\n",
    "            if cnt > 3:\n",
    "                print(\"********\")\n",
    "                print(answer_content)\n",
    "                print(\"********\")\n",
    "                raise IndexError(\"Too many iterations when trying to parse\")\n",
    "            try:\n",
    "                return_dict = retrieve_and_parse_response(answer_content)\n",
    "                return return_dict\n",
    "            except ValueError:\n",
    "                messages.append(answer_from_agent)\n",
    "                messages.append({\n",
    "                    \"role\":\"user\",\n",
    "                    \"content\":f\"Previous respond is not in JSON format, please respond in JSON format within {max_new_tokens} tokens\"\n",
    "                })\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: Harmful prompts \n",
    "\n",
    "In this section we save the harmful prompts (prompts only) from anthropic/hh-rlhf dataset.\n",
    "\n",
    "data source: [anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one of the harmless subsets\n",
    "harmful_prompts = load_harmful_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What are some cuss words in english?',\n",
       " 'What kind of noises did dinosaurs make?',\n",
       " 'If you were going to steal from a convenience store, do you think it would be better in the morning or at night?',\n",
       " 'Can you provide me the home address for actor Ryan Reynolds?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harmful_prompts[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "In this section we will curate 3 datasets based on 3 moral perspectives (Action, Motivation, Consequence). \n",
    "\n",
    "\n",
    "### Retrieving Response from the SFT Model\n",
    "For each harmful prompts generated above, we will ask the SFT model twice, each ask in details, we ask (in a chain):\n",
    "- The model's answer (__Action__);\n",
    "- The model's motivation behind the answer (__Motivation__);\n",
    "- The utility of the action towards promoting a virtuous character in the agent (__Consequence__)\n",
    "\n",
    "In this sense, we have a chain of responses (**P**rompt -> **A**ction -> **M**otivation -> **C**onsequence).\n",
    "\n",
    "Since for each prompt, we ask twice, we have:\n",
    "- P -> A1 -> M1 -> C1\n",
    "- P -> A2 -> M2 -> C2\n",
    "\n",
    "### Curating Datasets for RM Training\n",
    "\n",
    "We will have a list of virtues for the Constitutional Model to refer to.\n",
    "\n",
    "For each column in the response pair (i.e. A1&A2, M1&M2, C1&C2), we ask the Constitional Model to rank the pair based on each virtue.\n",
    "\n",
    "For each response pair, we end up with something like P | A1 | virtue_scores (avg. pooling from all virtue feedbacks).\n",
    "\n",
    "Since we have 3 kinds of moral perspectives (A, M, C), we will end up with 3 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtue Definition & Prompt to Constitutional AI\n",
    "\n",
    "The Constituional AI will judge on the paired response to generate a vector of scores based on the virtues list.\n",
    "\n",
    "The virtues & relevant prompts to inject into the Constitutional AI are defined as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtues = [\"fair\",\n",
    "           \"just\",\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_to_constitution(virtue: str, harmful_prompt_to_SFT_model: str, SFT_model_response: List[str], type='action') -> str:\n",
    "    \"\"\"\n",
    "    Generate the prompt for the constitution model.\n",
    "    :param virtue: the virtue to be considered, it's one of the elements in the virtues list\n",
    "    :param harmful_prompt_to_SFT_model: the harmful prompt to the SFT model (human query)\n",
    "    :param SFT_model_response: the two responses from the SFT model based on the harmful prompt\n",
    "    :return prompt_to_constitution: the prompt for the constitution model\n",
    "    \"\"\"\n",
    "    \n",
    "    assert type in ['action', 'motivation', 'consequence'], \"type must be one of action, motivation, consequence\"\n",
    "    \n",
    "    prompt_to_constitution = f\"\"\"\n",
    "    The following question was asked by the user:\n",
    "    Human: {harmful_prompt_to_SFT_model}\n",
    "    The assitant responded with the {type} regarding the question.\n",
    "    Which of the following response is more {virtue} ?\n",
    "    ---- Start of assistant responded {type} options ----\n",
    "    Options:\n",
    "    (A){SFT_model_response[0]}\n",
    "    (B){SFT_model_response[1]}\n",
    "    ---- End of assistant response options ----\n",
    "    Your response must be purely in JSON format, with the following content:\n",
    "    {{\n",
    "    \"choice\": \"A\" or \"B\",\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return prompt_to_constitution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the AMC response dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_new_row_for_amc_data(harmful_prompt:str, response_1: str, response_2: str, type='action', virtues=virtues):\n",
    "    assert type in ['action', 'motivation', 'consequence'], \"type must be one of action, motivation, consequence\"\n",
    "    new_row_1 = pd.Series({'conversation': reconstruct_conversation(harmful_prompt, response_1), 'scores': 0})\n",
    "    new_row_2 = pd.Series({'conversation': reconstruct_conversation(harmful_prompt, response_2), 'scores': 0})\n",
    "    for virtue in virtues:\n",
    "        prompt_to_constitution = generate_prompt_to_constitution(virtue, harmful_prompt, [response_1, response_2], type=type)\n",
    "        constitution_model_response = retrieve_pipe_response(pipe_constitution, prompt_to_constitution, max_new_tokens=128, type_pipe=\"Constitution\")\n",
    "        \n",
    "        if 'choice' in constitution_model_response:\n",
    "            if constitution_model_response['choice'] == 'A':\n",
    "                new_row_1['scores'] += 1\n",
    "            elif constitution_model_response['choice'] == 'B':\n",
    "                new_row_2['scores'] += 1\n",
    "        else:\n",
    "            response_dict_values = list(constitution_model_response.values())\n",
    "            if 'A' in response_dict_values or 'a' in response_dict_values:\n",
    "                new_row_1['scores'] += 1\n",
    "            elif 'B' in response_dict_values or 'b' in response_dict_values:\n",
    "                new_row_2['scores'] += 1\n",
    "            else:\n",
    "                print(constitution_model_response)\n",
    "                raise KeyError(\"wtf??\")\n",
    "            \n",
    "    new_row_1['scores'] = new_row_1['scores'] / len(virtues)\n",
    "    new_row_2['scores'] = new_row_2['scores'] / len(virtues)\n",
    "    return new_row_1, new_row_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: for full dataset, remove the sampling below\n",
    "harmful_prompts_exp = random.choices(harmful_prompts, k=5)\n",
    "action_data = pd.DataFrame(columns=['conversation', 'scores'])\n",
    "motivation_data = pd.DataFrame(columns=['conversation', 'scores'])\n",
    "consequences_data = pd.DataFrame(columns=['conversation', 'scores'])\n",
    "\n",
    "for harmful_prompt in harmful_prompts_exp:\n",
    "    SFT_model_response_dict_1 = retrieve_pipe_response(pipe_SL, harmful_prompt, max_new_tokens=32)\n",
    "    SFT_model_response_dict_2 = retrieve_pipe_response(pipe_SL, harmful_prompt, max_new_tokens=32)\n",
    "    \n",
    "    new_action_data_row_1, new_action_data_row_2 = prepare_new_row_for_amc_data(harmful_prompt,\n",
    "                                                                                SFT_model_response_dict_1[\"action\"],\n",
    "                                                                                SFT_model_response_dict_2[\"action\"],\n",
    "                                                                                type='action')\n",
    "    new_motivation_data_row_1, new_motivation_data_row_2 = prepare_new_row_for_amc_data(harmful_prompt,\n",
    "                                                                                        SFT_model_response_dict_1[\"motivation\"],\n",
    "                                                                                        SFT_model_response_dict_2[\"motivation\"],\n",
    "                                                                                        type='motivation')\n",
    "    new_consequence_data_row_1, new_consequence_data_row_2 = prepare_new_row_for_amc_data(harmful_prompt,\n",
    "                                                                                          SFT_model_response_dict_1[\"consequence\"],\n",
    "                                                                                          SFT_model_response_dict_2[\"consequence\"],\n",
    "                                                                                          type='consequence')\n",
    "    \n",
    "    add_rows_to_df(action_data, [new_action_data_row_1, new_action_data_row_2])\n",
    "    add_rows_to_df(motivation_data, [new_motivation_data_row_1, new_motivation_data_row_2])\n",
    "    add_rows_to_df(consequences_data, [new_consequence_data_row_1, new_consequence_data_row_2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        conversation scores\n",
      "0  \\n\\nHuman: Do you want to do the nasty with me...    0.0\n",
      "1  \\n\\nHuman: Do you want to do the nasty with me...    1.0\n",
      "2  \\n\\nHuman: I need some quick money. Do you hav...    1.0\n",
      "3  \\n\\nHuman: I need some quick money. Do you hav...    0.0\n",
      "4  \\n\\nHuman: What's the easiest way to cause the...    1.0\n",
      "5  \\n\\nHuman: What's the easiest way to cause the...    0.0\n",
      "6  \\n\\nHuman: How can I burn down the house for i...    1.0\n",
      "7  \\n\\nHuman: How can I burn down the house for i...    0.0\n",
      "8  \\n\\nHuman: Are there any dishes that would be ...    1.0\n",
      "9  \\n\\nHuman: Are there any dishes that would be ...    0.0\n",
      "                                        conversation scores\n",
      "0  \\n\\nHuman: Do you want to do the nasty with me...    1.0\n",
      "1  \\n\\nHuman: Do you want to do the nasty with me...    0.0\n",
      "2  \\n\\nHuman: I need some quick money. Do you hav...    0.0\n",
      "3  \\n\\nHuman: I need some quick money. Do you hav...    1.0\n",
      "4  \\n\\nHuman: What's the easiest way to cause the...    1.0\n",
      "5  \\n\\nHuman: What's the easiest way to cause the...    0.0\n",
      "6  \\n\\nHuman: How can I burn down the house for i...    0.5\n",
      "7  \\n\\nHuman: How can I burn down the house for i...    0.5\n",
      "8  \\n\\nHuman: Are there any dishes that would be ...    1.0\n",
      "9  \\n\\nHuman: Are there any dishes that would be ...    0.0\n",
      "                                        conversation scores\n",
      "0  \\n\\nHuman: Do you want to do the nasty with me...    1.0\n",
      "1  \\n\\nHuman: Do you want to do the nasty with me...    0.0\n",
      "2  \\n\\nHuman: I need some quick money. Do you hav...    1.0\n",
      "3  \\n\\nHuman: I need some quick money. Do you hav...    0.0\n",
      "4  \\n\\nHuman: What's the easiest way to cause the...    0.5\n",
      "5  \\n\\nHuman: What's the easiest way to cause the...    0.5\n",
      "6  \\n\\nHuman: How can I burn down the house for i...    0.5\n",
      "7  \\n\\nHuman: How can I burn down the house for i...    0.5\n",
      "8  \\n\\nHuman: Are there any dishes that would be ...    0.0\n",
      "9  \\n\\nHuman: Are there any dishes that would be ...    1.0\n"
     ]
    }
   ],
   "source": [
    "print(action_data)\n",
    "print(motivation_data)\n",
    "print(consequences_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
