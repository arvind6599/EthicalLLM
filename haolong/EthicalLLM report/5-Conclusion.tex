In this project, we designed distinct preference models tailored to each evaluative criterion. These models were subsequently utilized in the reinforcement learning training process of a large language model (LLM) assistant. This approach aimed to holistically integrate virtue ethics into the operational dynamics of the LLM, ensuring that its functionality aligns with the ethical tenets of virtue ethics.

We implemented RLAIF with our own curated datasets representing preferences on each virtue ethics. As a result, we obtain a final fine-tuned model. However, we observed a non-increased model performance with our evaluation metric implemented with the reward models and qualitative analysis. We conclude this may come from incomplete conversation in the reward model training dataset and lack of the SFT model, leading to the model's decreased ability to answer questions coherently in general.